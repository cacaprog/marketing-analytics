{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create web analytics dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dirty Website Analytics Data Generated:\n",
      "         date  page_views  unique_visitors  sessions  bounce_rate  \\\n",
      "0  2024-07-21       14434            36241     56102        25.62   \n",
      "1  2024-07-22       21926            22798     29021        40.20   \n",
      "2  2024-07-23       50615            19713     48200        62.27   \n",
      "3  2024-07-24       47566            17996     10158        30.27   \n",
      "4  2024-07-25       53581            14326     75141        38.88   \n",
      "\n",
      "   average_session_duration traffic_source device_type    country  conversions  \n",
      "0                        57           None      Mobile      Japan          743  \n",
      "1                       129          Email     Desktop      China          362  \n",
      "2                       263           None     Desktop  Australia           91  \n",
      "3                       197         FB Ads      Tablet      Japan          286  \n",
      "4                       435        Organic     Desktop     Brazil          668  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86239/2425329212.py:107: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  website_analytics_df['parsed_date'] = pd.to_datetime(website_analytics_df['date'], dayfirst=True, errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "fake.seed_instance(42)\n",
    "\n",
    "# Parameters\n",
    "num_days = 90  # Approximately 3 months\n",
    "total_records = num_days  # One record per day\n",
    "\n",
    "# Define traffic sources and device types with some inconsistencies\n",
    "traffic_sources = ['Organic', 'Direct', 'Referral', 'Social', 'Email', 'Paid Search', 'Display Ads', 'Social Media', 'Organic Search']\n",
    "device_types = ['Desktop', 'Mobile', 'Tablet', 'desktop', 'mobile', 'tab']\n",
    "countries = ['United States', 'Canada', 'United Kingdom', 'Australia', 'Germany', 'France', 'India', 'Brazil', 'Japan', 'China']\n",
    "\n",
    "# Define a list of platforms from the advertising data to align traffic sources\n",
    "ad_platforms = ['Google Ads', 'Facebook Ads', 'Instagram Ads', 'LinkedIn Ads', 'GAds', 'FB Ads', 'Insta Ads']\n",
    "\n",
    "# Generate a list of dates for the past 3 months\n",
    "date_range = pd.date_range(end=pd.Timestamp.today(), periods=num_days).tolist()\n",
    "\n",
    "# Initialize list to hold all website analytics records\n",
    "website_analytics = []\n",
    "\n",
    "for single_date in date_range:\n",
    "    # Introduce inconsistent date formats (10% of the time)\n",
    "    if random.random() > 0.1:\n",
    "        date_str = single_date.strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        date_str = single_date.strftime('%d/%m/%Y')\n",
    "    \n",
    "    # Introduce missing values (5% for 'traffic_source' and 'device_type')\n",
    "    traffic_source = random.choice(traffic_sources) if random.random() > 0.05 else None\n",
    "    device_type = random.choice(device_types) if random.random() > 0.05 else None\n",
    "    \n",
    "    # Introduce erroneous entries (2% negative values for numeric metrics)\n",
    "    page_views = random.randint(1000, 100000) if random.random() > 0.02 else -random.randint(1000, 100000)\n",
    "    unique_visitors = random.randint(500, 50000) if random.random() > 0.02 else -random.randint(500, 50000)\n",
    "    sessions = random.randint(800, 80000) if random.random() > 0.02 else -random.randint(800, 80000)\n",
    "    bounce_rate = round(random.uniform(20, 80), 2) if random.random() > 0.02 else -round(random.uniform(20, 80), 2)\n",
    "    average_session_duration = random.randint(30, 600) if random.random() > 0.02 else -random.randint(30, 600)\n",
    "    conversions = random.randint(10, 1000) if random.random() > 0.02 else -random.randint(10, 1000)\n",
    "    \n",
    "    # Assign traffic source from advertising platforms with some probability\n",
    "    # This introduces a relation between ad platforms and traffic sources\n",
    "    if traffic_source in ['Social Media', 'Paid Search', 'Display Ads']:\n",
    "        traffic_source = random.choice(ad_platforms)\n",
    "    \n",
    "    # Assign country\n",
    "    country = random.choice(countries)\n",
    "    \n",
    "    # Assign device type with consistency\n",
    "    device_type = device_type.capitalize() if device_type else device_type\n",
    "    \n",
    "    # Create the website analytics record\n",
    "    analytics_record = {\n",
    "        'date': date_str,\n",
    "        'page_views': page_views,\n",
    "        'unique_visitors': unique_visitors,\n",
    "        'sessions': sessions,\n",
    "        'bounce_rate': bounce_rate,\n",
    "        'average_session_duration': average_session_duration,\n",
    "        'traffic_source': traffic_source,\n",
    "        'device_type': device_type,\n",
    "        'country': country,\n",
    "        'conversions': conversions\n",
    "    }\n",
    "    \n",
    "    website_analytics.append(analytics_record)\n",
    "\n",
    "# Convert to DataFrame\n",
    "website_analytics_df = pd.DataFrame(website_analytics)\n",
    "\n",
    "# Introduce Duplicates (2% of the data)\n",
    "duplicates = website_analytics_df.sample(frac=0.02, random_state=42)\n",
    "website_analytics_df = pd.concat([website_analytics_df, duplicates], ignore_index=True)\n",
    "\n",
    "# Introduce Additional Outliers by adding some extreme values (20 records)\n",
    "outlier_records = []\n",
    "for _ in range(20):\n",
    "    outlier_date = random.choice(date_range).strftime('%Y-%m-%d')\n",
    "    outlier_record = {\n",
    "        'date': outlier_date,\n",
    "        'page_views': random.randint(1000000, 10000000),  # Extremely high page views\n",
    "        'unique_visitors': random.randint(500000, 5000000),  # Extremely high unique visitors\n",
    "        'sessions': random.randint(800000, 8000000),  # Extremely high sessions\n",
    "        'bounce_rate': round(random.uniform(5, 95), 2),  # Extreme bounce rates\n",
    "        'average_session_duration': random.randint(10, 3600),  # Extremely long session durations\n",
    "        'traffic_source': random.choice(ad_platforms),\n",
    "        'device_type': random.choice(['Desktop', 'Mobile', 'Tablet']),\n",
    "        'country': random.choice(countries),\n",
    "        'conversions': random.randint(1000, 10000)  # Extremely high conversions\n",
    "    }\n",
    "    outlier_records.append(outlier_record)\n",
    "\n",
    "# Convert outliers to DataFrame\n",
    "outliers_df = pd.DataFrame(outlier_records)\n",
    "\n",
    "# Concatenate outliers with the main DataFrame\n",
    "website_analytics_df = pd.concat([website_analytics_df, outliers_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the DataFrame to mix outliers and duplicates\n",
    "website_analytics_df = website_analytics_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Parse 'date' column to datetime for sorting\n",
    "website_analytics_df['parsed_date'] = pd.to_datetime(website_analytics_df['date'], dayfirst=True, errors='coerce')\n",
    "\n",
    "# Sort by 'parsed_date'\n",
    "website_analytics_df = website_analytics_df.sort_values(by='parsed_date').drop('parsed_date', axis=1).reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "website_analytics_df.to_csv('website_analytics_dirty.csv', index=False)\n",
    "print(\"Dirty Website Analytics Data Generated:\")\n",
    "print(website_analytics_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CRM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dirty CRM Data Generated:\n",
      "  customer_id first_name last_name                        email  \\\n",
      "0    cust_185    Katelyn     Nunez         justin07@example.net   \n",
      "1     cust_22     Angela    Garcia       lauradixon@example.org   \n",
      "2    cust_961      Keith    Taylor  cervantessandra@example.net   \n",
      "3    cust_491     Olivia    Murphy      gentrylarry@example.com   \n",
      "4    cust_352       Sean    Sparks      isaiahbrown@example.net   \n",
      "\n",
      "           phone_number                    address                 city  \\\n",
      "0  001-296-277-8889x221  04094 Wolf Cliff Apt. 246       Lake Scottfort   \n",
      "1    (732)978-6807x0423          89959 Chad Common       Maldonadoshire   \n",
      "2    (551)681-2759x1700     4325 Shaffer Mountains         Anthonymouth   \n",
      "3  001-291-753-2180x113         15682 Keith Plains            Sarahtown   \n",
      "4   +1-597-503-2043x474   2106 Jones Cape Apt. 460  North Cristinaville   \n",
      "\n",
      "        state         country zip_code signup_date last_purchase_date  \\\n",
      "0        Ohio         Germany    91314  2019-10-29         2021-09-07   \n",
      "1       Idaho   United States    88904  2019-12-16         2020-04-22   \n",
      "2     Indiana           Japan    50300  2020-01-26         2020-03-23   \n",
      "3  Washington  United Kingdom    16935  2020-02-02         2021-04-30   \n",
      "4    Illinois           India    16486  2020-04-06         2020-10-28   \n",
      "\n",
      "   total_purchases  total_spent loyalty_program_member customer_segment  \\\n",
      "0              522     45130.07                    Yes         Platinum   \n",
      "1              513     81889.06                    Yes         Platinum   \n",
      "2              605     59594.34                    Yes         Platinum   \n",
      "3              638     77866.38                    Yes         Platinum   \n",
      "4              970     24633.80                    Yes         Platinum   \n",
      "\n",
      "  preferred_channel  feedback_score  support_tickets_opened churn_risk  \n",
      "0             Email               5                      43       High  \n",
      "1             Email               5                      17       High  \n",
      "2             Email               5                      27       High  \n",
      "3             Email               5                      11       High  \n",
      "4             Email               5                      19       High  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86239/343986587.py:197: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  crm_df['parsed_signup_date'] = pd.to_datetime(crm_df['signup_date'], dayfirst=True, errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "fake.seed_instance(42)\n",
    "\n",
    "# Parameters\n",
    "num_customers = 1000  # Total number of customers\n",
    "total_records = num_customers\n",
    "\n",
    "# Define customer segments, loyalty program status, and preferred channels\n",
    "customer_segments = ['Bronze', 'Silver', 'Gold', 'Platinum']\n",
    "loyalty_program_status = ['Yes', 'No']\n",
    "preferred_channels = ['Email', 'Phone', 'SMS', 'None']\n",
    "\n",
    "# Define countries (consistent with other datasets)\n",
    "countries = ['United States', 'Canada', 'United Kingdom', 'Australia', 'Germany', 'France',\n",
    "             'India', 'Brazil', 'Japan', 'China']\n",
    "\n",
    "# Generate a list of dates for the past 3 years (assuming CRM data spans longer)\n",
    "date_range = pd.date_range(end=pd.Timestamp.today(), periods=1095).tolist()  # Approximately 3 years\n",
    "\n",
    "# Initialize list to hold all CRM records\n",
    "crm_records = []\n",
    "\n",
    "for customer_num in range(1, num_customers + 1):\n",
    "    customer_id = f'cust_{customer_num}'\n",
    "    first_name = fake.first_name()\n",
    "    last_name = fake.last_name()\n",
    "    email = fake.email()\n",
    "    phone_number = fake.phone_number()\n",
    "    address = fake.street_address()\n",
    "    city = fake.city()\n",
    "    state = fake.state()\n",
    "    country = random.choice(countries)\n",
    "    zip_code = fake.zipcode()\n",
    "    \n",
    "    # Generate signup date within the past 3 years\n",
    "    signup_date = fake.date_between(start_date='-3y', end_date='today')\n",
    "    \n",
    "    # Generate last purchase date after signup_date\n",
    "    last_purchase_date = fake.date_between(start_date=signup_date, end_date='today')\n",
    "    \n",
    "    # Total purchases between 1 and 100\n",
    "    total_purchases = random.randint(1, 100)\n",
    "    \n",
    "    # Total spent between $10 and $10,000\n",
    "    total_spent = round(random.uniform(10, 10000), 2)\n",
    "    \n",
    "    # Loyalty program member\n",
    "    loyalty_member = random.choice(loyalty_program_status)\n",
    "    \n",
    "    # Customer segment based on total_spent\n",
    "    if total_spent < 100:\n",
    "        segment = 'Bronze'\n",
    "    elif 100 <= total_spent < 500:\n",
    "        segment = 'Silver'\n",
    "    elif 500 <= total_spent < 2000:\n",
    "        segment = 'Gold'\n",
    "    else:\n",
    "        segment = 'Platinum'\n",
    "    \n",
    "    # Preferred communication channel\n",
    "    preferred_channel = random.choice(preferred_channels)\n",
    "    \n",
    "    # Feedback score between 1 and 5\n",
    "    feedback_score = random.randint(1, 5)\n",
    "    \n",
    "    # Support tickets opened between 0 and 20\n",
    "    support_tickets = random.randint(0, 20)\n",
    "    \n",
    "    # Churn risk based on last_purchase_date\n",
    "    days_since_last_purchase = (pd.Timestamp.today() - pd.Timestamp(last_purchase_date)).days\n",
    "    if days_since_last_purchase < 30:\n",
    "        churn_risk = 'Low'\n",
    "    elif 30 <= days_since_last_purchase < 90:\n",
    "        churn_risk = 'Medium'\n",
    "    else:\n",
    "        churn_risk = 'High'\n",
    "    \n",
    "    # Introduce missing values (5% for 'phone_number', 'preferred_channel')\n",
    "    phone_number = phone_number if random.random() > 0.05 else None\n",
    "    preferred_channel = preferred_channel if random.random() > 0.05 else None\n",
    "    \n",
    "    # Introduce erroneous entries (2% negative values for 'total_purchases', 'total_spent')\n",
    "    total_purchases = total_purchases if random.random() > 0.02 else -total_purchases\n",
    "    total_spent = total_spent if random.random() > 0.02 else -total_spent\n",
    "    \n",
    "    # Create the CRM record\n",
    "    crm_record = {\n",
    "        'customer_id': customer_id,\n",
    "        'first_name': first_name,\n",
    "        'last_name': last_name,\n",
    "        'email': email,\n",
    "        'phone_number': phone_number,\n",
    "        'address': address,\n",
    "        'city': city,\n",
    "        'state': state,\n",
    "        'country': country,\n",
    "        'zip_code': zip_code,\n",
    "        'signup_date': signup_date.strftime('%Y-%m-%d'),\n",
    "        'last_purchase_date': last_purchase_date.strftime('%Y-%m-%d'),\n",
    "        'total_purchases': total_purchases,\n",
    "        'total_spent': total_spent,\n",
    "        'loyalty_program_member': loyalty_member,\n",
    "        'customer_segment': segment,\n",
    "        'preferred_channel': preferred_channel,\n",
    "        'feedback_score': feedback_score,\n",
    "        'support_tickets_opened': support_tickets,\n",
    "        'churn_risk': churn_risk\n",
    "    }\n",
    "    \n",
    "    crm_records.append(crm_record)\n",
    "\n",
    "# Convert to DataFrame\n",
    "crm_df = pd.DataFrame(crm_records)\n",
    "\n",
    "# Introduce Duplicates (2% of the data)\n",
    "duplicates = crm_df.sample(frac=0.02, random_state=42)\n",
    "crm_df = pd.concat([crm_df, duplicates], ignore_index=True)\n",
    "\n",
    "# Introduce Additional Outliers by adding some extreme values (20 records)\n",
    "outlier_records = []\n",
    "for _ in range(20):\n",
    "    outlier_customer_num = random.randint(1, num_customers)\n",
    "    outlier_customer_id = f'cust_{outlier_customer_num}'\n",
    "    outlier_first_name = fake.first_name()\n",
    "    outlier_last_name = fake.last_name()\n",
    "    outlier_email = fake.email()\n",
    "    outlier_phone_number = fake.phone_number()\n",
    "    outlier_address = fake.street_address()\n",
    "    outlier_city = fake.city()\n",
    "    outlier_state = fake.state()\n",
    "    outlier_country = random.choice(countries)\n",
    "    outlier_zip_code = fake.zipcode()\n",
    "    \n",
    "    # Extreme signup and last purchase dates\n",
    "    outlier_signup_date = fake.date_between(start_date='-5y', end_date='-3y')\n",
    "    outlier_last_purchase_date = fake.date_between(start_date=outlier_signup_date, end_date='-3y')\n",
    "    \n",
    "    # Extremely high total purchases and total spent\n",
    "    outlier_total_purchases = random.randint(500, 1000)\n",
    "    outlier_total_spent = round(random.uniform(20000, 100000), 2)\n",
    "    \n",
    "    # Loyalty program member\n",
    "    outlier_loyalty_member = 'Yes'\n",
    "    \n",
    "    # Customer segment\n",
    "    outlier_segment = 'Platinum'\n",
    "    \n",
    "    # Preferred communication channel\n",
    "    outlier_preferred_channel = 'Email'\n",
    "    \n",
    "    # Feedback score\n",
    "    outlier_feedback_score = 5\n",
    "    \n",
    "    # Support tickets opened\n",
    "    outlier_support_tickets = random.randint(5, 50)\n",
    "    \n",
    "    # Churn risk\n",
    "    outlier_churn_risk = 'High'\n",
    "    \n",
    "    # Create the outlier CRM record\n",
    "    outlier_record = {\n",
    "        'customer_id': outlier_customer_id,\n",
    "        'first_name': outlier_first_name,\n",
    "        'last_name': outlier_last_name,\n",
    "        'email': outlier_email,\n",
    "        'phone_number': outlier_phone_number,\n",
    "        'address': outlier_address,\n",
    "        'city': outlier_city,\n",
    "        'state': outlier_state,\n",
    "        'country': outlier_country,\n",
    "        'zip_code': outlier_zip_code,\n",
    "        'signup_date': outlier_signup_date.strftime('%Y-%m-%d'),\n",
    "        'last_purchase_date': outlier_last_purchase_date.strftime('%Y-%m-%d'),\n",
    "        'total_purchases': outlier_total_purchases,\n",
    "        'total_spent': outlier_total_spent,\n",
    "        'loyalty_program_member': outlier_loyalty_member,\n",
    "        'customer_segment': outlier_segment,\n",
    "        'preferred_channel': outlier_preferred_channel,\n",
    "        'feedback_score': outlier_feedback_score,\n",
    "        'support_tickets_opened': outlier_support_tickets,\n",
    "        'churn_risk': outlier_churn_risk\n",
    "    }\n",
    "    \n",
    "    outlier_records.append(outlier_record)\n",
    "\n",
    "# Convert outliers to DataFrame\n",
    "outliers_df = pd.DataFrame(outlier_records)\n",
    "\n",
    "# Concatenate outliers with the main DataFrame\n",
    "crm_df = pd.concat([crm_df, outliers_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the DataFrame to mix outliers and duplicates\n",
    "crm_df = crm_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Parse 'signup_date' column to datetime for sorting\n",
    "crm_df['parsed_signup_date'] = pd.to_datetime(crm_df['signup_date'], dayfirst=True, errors='coerce')\n",
    "\n",
    "# Sort by 'parsed_signup_date'\n",
    "crm_df = crm_df.sort_values(by='parsed_signup_date').drop('parsed_signup_date', axis=1).reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "crm_df.to_csv('crm_dirty.csv', index=False)\n",
    "print(\"Dirty CRM Data Generated:\")\n",
    "print(crm_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Email campaigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dirty Email Campaigns Data Generated:\n",
      "   campaign_id                   campaign_name   date_sent  \\\n",
      "0   campaign_2  Campaign 2 - Brother Promotion  2024-08-01   \n",
      "1  campaign_18  Campaign 18 - Almost Promotion  2024-08-01   \n",
      "2  campaign_43    Campaign 43 - Save Promotion  2024-08-01   \n",
      "3  campaign_24    Campaign 24 - Play Promotion  2024-08-01   \n",
      "4  campaign_38    Campaign 38 - True Promotion  2024-08-01   \n",
      "\n",
      "                   subject_line      platform         country  \\\n",
      "0         Your Feedback Matters          GAds  United Kingdom   \n",
      "1     Welcome to Our Newsletter    Google Ads           India   \n",
      "2    New Arrivals Available Now        FB Ads          France   \n",
      "3  Unlock Your Premium Features  LinkedIn Ads          France   \n",
      "4         Your Feedback Matters  LinkedIn Ads           China   \n",
      "\n",
      "   recipient_count  delivered_count  opened_count  clicked_count  \\\n",
      "0            49368            35694         21778          14918   \n",
      "1            25670             7756          1289            942   \n",
      "2             7461             5927          5007            529   \n",
      "3             8339             2404          2390            366   \n",
      "4            91308            22793           308            300   \n",
      "\n",
      "   bounced_count  unsubscribe_count  spam_report_count  conversions  \n",
      "0            257                369                 30          983  \n",
      "1            878                 97                197          495  \n",
      "2            799                184                 97           55  \n",
      "3            479                334                 97          832  \n",
      "4            221                454                 22          244  \n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "fake.seed_instance(42)\n",
    "\n",
    "# Parameters\n",
    "num_campaigns = 50  # Total number of email campaigns\n",
    "records_per_campaign = 20  # Number of records per campaign (simulating multiple sends or variations)\n",
    "total_records = num_campaigns * records_per_campaign\n",
    "\n",
    "# Define possible subject lines, platforms, and other categorical variables\n",
    "subject_lines = [\n",
    "    \"Exclusive Offer Just for You!\",\n",
    "    \"Don't Miss Out on Our Summer Sale\",\n",
    "    \"Welcome to Our Newsletter\",\n",
    "    \"Limited Time Discount Inside\",\n",
    "    \"Your Feedback Matters\",\n",
    "    \"New Arrivals Available Now\",\n",
    "    \"Special Invitation: Join Us Today\",\n",
    "    \"Thank You for Being a Loyal Customer\",\n",
    "    \"Unlock Your Premium Features\",\n",
    "    \"Last Chance to Save Big!\"\n",
    "]\n",
    "\n",
    "platforms = ['Google Ads', 'Facebook Ads', 'Instagram Ads', 'LinkedIn Ads', 'GAds', 'FB Ads', 'Insta Ads', 'Email Campaign']\n",
    "countries = ['United States', 'Canada', 'United Kingdom', 'Australia', 'Germany', 'France', 'India', 'Brazil', 'Japan', 'China']\n",
    "\n",
    "# Generate a list of dates for the past 3 months\n",
    "date_range = pd.date_range(end=pd.Timestamp.today(), periods=90).tolist()\n",
    "\n",
    "# Initialize list to hold all email campaign records\n",
    "email_campaigns = []\n",
    "\n",
    "for campaign_num in range(1, num_campaigns + 1):\n",
    "    campaign_id = f'campaign_{campaign_num}'\n",
    "    campaign_name = f'Campaign {campaign_num} - {fake.word().capitalize()} Promotion'\n",
    "    \n",
    "    for _ in range(records_per_campaign):\n",
    "        # Introduce inconsistent date formats (10% of the time)\n",
    "        if random.random() > 0.1:\n",
    "            date_sent = random.choice(date_range).strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            date_sent = random.choice(date_range).strftime('%d/%m/%Y')\n",
    "        \n",
    "        # Select a subject line\n",
    "        subject_line = random.choice(subject_lines)\n",
    "        \n",
    "        # Introduce missing values (5% for 'subject_line' and 'platform')\n",
    "        subject_line = subject_line if random.random() > 0.05 else None\n",
    "        platform = random.choice(platforms) if random.random() > 0.05 else None\n",
    "        \n",
    "        # Introduce erroneous entries (2% negative values for numeric metrics)\n",
    "        recipient_count = random.randint(1000, 100000) if random.random() > 0.02 else -random.randint(1000, 100000)\n",
    "        delivered_count = random.randint(900, recipient_count) if (recipient_count > 0 and random.random() > 0.02) else -random.randint(900, recipient_count if recipient_count > 0 else 1000)\n",
    "        opened_count = random.randint(100, delivered_count) if (delivered_count > 0 and random.random() > 0.02) else -random.randint(100, delivered_count if delivered_count > 0 else 1000)\n",
    "        clicked_count = random.randint(50, opened_count) if (opened_count > 0 and random.random() > 0.02) else -random.randint(50, opened_count if opened_count > 0 else 1000)\n",
    "        bounced_count = random.randint(0, 1000) if random.random() > 0.02 else -random.randint(0, 1000)\n",
    "        unsubscribe_count = random.randint(0, 500) if random.random() > 0.02 else -random.randint(0, 500)\n",
    "        spam_report_count = random.randint(0, 200) if random.random() > 0.02 else -random.randint(0, 200)\n",
    "        conversions = random.randint(10, 1000) if random.random() > 0.02 else -random.randint(10, 1000)\n",
    "        \n",
    "        # Assign country\n",
    "        country = random.choice(countries)\n",
    "        \n",
    "        # Create the email campaign record\n",
    "        campaign_record = {\n",
    "            'campaign_id': campaign_id,\n",
    "            'campaign_name': campaign_name,\n",
    "            'date_sent': date_sent,\n",
    "            'subject_line': subject_line,\n",
    "            'platform': platform,\n",
    "            'country': country,\n",
    "            'recipient_count': recipient_count,\n",
    "            'delivered_count': delivered_count,\n",
    "            'opened_count': opened_count,\n",
    "            'clicked_count': clicked_count,\n",
    "            'bounced_count': bounced_count,\n",
    "            'unsubscribe_count': unsubscribe_count,\n",
    "            'spam_report_count': spam_report_count,\n",
    "            'conversions': conversions\n",
    "        }\n",
    "        \n",
    "        email_campaigns.append(campaign_record)\n",
    "\n",
    "# Convert to DataFrame\n",
    "email_campaigns_df = pd.DataFrame(email_campaigns)\n",
    "\n",
    "# Introduce Duplicates (2% of the data)\n",
    "duplicates = email_campaigns_df.sample(frac=0.02, random_state=42)\n",
    "email_campaigns_df = pd.concat([email_campaigns_df, duplicates], ignore_index=True)\n",
    "\n",
    "# Introduce Additional Outliers by adding some extreme values (10 records)\n",
    "outlier_records = []\n",
    "for _ in range(10):\n",
    "    outlier_campaign_num = random.randint(1, num_campaigns)\n",
    "    outlier_campaign_id = f'campaign_{outlier_campaign_num}'\n",
    "    outlier_campaign_name = f'Campaign {outlier_campaign_num} - {fake.word().capitalize()} Blitz'\n",
    "    outlier_date = random.choice(date_range).strftime('%Y-%m-%d')\n",
    "    \n",
    "    outlier_record = {\n",
    "        'campaign_id': outlier_campaign_id,\n",
    "        'campaign_name': outlier_campaign_name,\n",
    "        'date_sent': outlier_date,\n",
    "        'subject_line': \"Super Sale! Unbeatable Prices Inside!\",\n",
    "        'platform': random.choice(platforms),\n",
    "        'country': random.choice(countries),\n",
    "        'recipient_count': random.randint(100000, 1000000),  # Extremely high recipient count\n",
    "        'delivered_count': random.randint(90000, 900000),    # Extremely high delivered count\n",
    "        'opened_count': random.randint(50000, 500000),        # Extremely high opened count\n",
    "        'clicked_count': random.randint(10000, 100000),       # Extremely high clicked count\n",
    "        'bounced_count': random.randint(0, 5000),             # High but plausible\n",
    "        'unsubscribe_count': random.randint(0, 1000),         # High but plausible\n",
    "        'spam_report_count': random.randint(0, 500),          # High but plausible\n",
    "        'conversions': random.randint(1000, 10000)            # Extremely high conversions\n",
    "    }\n",
    "    outlier_records.append(outlier_record)\n",
    "\n",
    "# Convert outliers to DataFrame\n",
    "outliers_df = pd.DataFrame(outlier_records)\n",
    "\n",
    "# Concatenate outliers with the main DataFrame\n",
    "email_campaigns_df = pd.concat([email_campaigns_df, outliers_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the DataFrame to mix outliers and duplicates\n",
    "email_campaigns_df = email_campaigns_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Parse 'date_sent' column to datetime for sorting\n",
    "email_campaigns_df['parsed_date'] = pd.to_datetime(email_campaigns_df['date_sent'], dayfirst=True, errors='coerce')\n",
    "\n",
    "# Sort by 'parsed_date'\n",
    "email_campaigns_df = email_campaigns_df.sort_values(by='parsed_date').drop('parsed_date', axis=1).reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "email_campaigns_df.to_csv('email_campaigns_dirty.csv', index=False)\n",
    "print(\"Dirty Email Campaigns Data Generated:\")\n",
    "print(email_campaigns_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dirty Social Media Data Generated:\n",
      "    post_id   platform content_type   content_theme date_posted  \\\n",
      "0  post_198   LinkedIn         Text      Engagement  21/07/2024   \n",
      "1  post_231    Twitter         Live  Product Launch  27/07/2024   \n",
      "2   post_13    Twitter        Story  User-Generated  31/07/2024   \n",
      "3   post_91  Instagram         Text           Event  01/08/2024   \n",
      "4   post_11     TikTok        Story  Product Launch  01/08/2024   \n",
      "\n",
      "   organic_impressions  paid_impressions  organic_engagements  \\\n",
      "0                91146             26794                 2635   \n",
      "1                38707            -45468                 5099   \n",
      "2                27772             20590                 6584   \n",
      "3                 5888             11473                 2980   \n",
      "4                29016              3842                  135   \n",
      "\n",
      "   paid_engagements  clicks  reach  followers_gain  conversions  \\\n",
      "0              4795    1760  75012             141          203   \n",
      "1               186    1525  27712             384          247   \n",
      "2              2457    1912  10508            -318           51   \n",
      "3               297     823  25420             492          148   \n",
      "4              2172    1617  56444             374          284   \n",
      "\n",
      "   sentiment_score        country  \n",
      "0            -0.99      Australia  \n",
      "1            -0.70           None  \n",
      "2             0.05           None  \n",
      "3             0.48  United States  \n",
      "4            -0.67           None  \n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "fake.seed_instance(42)\n",
    "\n",
    "# Parameters\n",
    "num_posts = 300  # Total number of social media posts\n",
    "records_per_post = 1  # One record per post\n",
    "total_records = num_posts * records_per_post\n",
    "\n",
    "# Define possible platforms, content types, and content themes\n",
    "platforms = ['Facebook', 'Twitter', 'Instagram', 'LinkedIn', 'Pinterest', 'TikTok']\n",
    "content_types = ['Image', 'Video', 'Text', 'Carousel', 'Story', 'Live']\n",
    "content_themes = ['Promotion', 'Engagement', 'Information', 'Behind-the-Scenes', 'User-Generated', 'Event', 'Product Launch']\n",
    "\n",
    "# Define countries if needed for geo-targeting (optional)\n",
    "countries = ['United States', 'Canada', 'United Kingdom', 'Australia', 'Germany', 'France', 'India', 'Brazil', 'Japan', 'China']\n",
    "\n",
    "# Generate a list of dates for the past 3 months\n",
    "date_range = pd.date_range(end=pd.Timestamp.today(), periods=90).tolist()\n",
    "\n",
    "# Initialize list to hold all social media records\n",
    "social_media_posts = []\n",
    "\n",
    "for post_num in range(1, num_posts + 1):\n",
    "    post_id = f'post_{post_num}'\n",
    "    platform = random.choice(platforms)\n",
    "    content_type = random.choice(content_types)\n",
    "    content_theme = random.choice(content_themes)\n",
    "    \n",
    "    # Introduce inconsistent date formats (10% of the time)\n",
    "    if random.random() > 0.1:\n",
    "        date_posted = random.choice(date_range).strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        date_posted = random.choice(date_range).strftime('%d/%m/%Y')\n",
    "    \n",
    "    # Introduce missing values (5% for 'content_theme' and 'sentiment_score')\n",
    "    content_theme = content_theme if random.random() > 0.05 else None\n",
    "    sentiment_score = round(random.uniform(-1, 1), 2) if random.random() > 0.05 else None  # -1 (negative) to 1 (positive)\n",
    "    \n",
    "    # Introduce erroneous entries (2% negative values for numeric metrics)\n",
    "    organic_impressions = random.randint(1000, 100000) if random.random() > 0.02 else -random.randint(1000, 100000)\n",
    "    paid_impressions = random.randint(0, 50000) if random.random() > 0.02 else -random.randint(0, 50000)\n",
    "    organic_engagements = random.randint(100, 10000) if random.random() > 0.02 else -random.randint(100, 10000)\n",
    "    paid_engagements = random.randint(0, 5000) if random.random() > 0.02 else -random.randint(0, 5000)\n",
    "    clicks = random.randint(10, 2000) if random.random() > 0.02 else -random.randint(10, 2000)\n",
    "    reach = random.randint(1000, 100000) if random.random() > 0.02 else -random.randint(1000, 100000)\n",
    "    followers_gain = random.randint(0, 500) if random.random() > 0.02 else -random.randint(0, 500)\n",
    "    conversions = random.randint(0, 300) if random.random() > 0.02 else -random.randint(0, 300)\n",
    "    \n",
    "    # Assign country (optional, based on platform or content)\n",
    "    country = random.choice(countries) if platform in ['Facebook', 'Instagram', 'LinkedIn'] else None\n",
    "    \n",
    "    # Create the social media post record\n",
    "    post_record = {\n",
    "        'post_id': post_id,\n",
    "        'platform': platform,\n",
    "        'content_type': content_type,\n",
    "        'content_theme': content_theme,\n",
    "        'date_posted': date_posted,\n",
    "        'organic_impressions': organic_impressions,\n",
    "        'paid_impressions': paid_impressions,\n",
    "        'organic_engagements': organic_engagements,\n",
    "        'paid_engagements': paid_engagements,\n",
    "        'clicks': clicks,\n",
    "        'reach': reach,\n",
    "        'followers_gain': followers_gain,\n",
    "        'conversions': conversions,\n",
    "        'sentiment_score': sentiment_score,\n",
    "        'country': country\n",
    "    }\n",
    "    \n",
    "    social_media_posts.append(post_record)\n",
    "\n",
    "# Convert to DataFrame\n",
    "social_media_df = pd.DataFrame(social_media_posts)\n",
    "\n",
    "# Introduce Duplicates (2% of the data)\n",
    "duplicates = social_media_df.sample(frac=0.02, random_state=42)\n",
    "social_media_df = pd.concat([social_media_df, duplicates], ignore_index=True)\n",
    "\n",
    "# Introduce Additional Outliers by adding some extreme values (15 records)\n",
    "outlier_records = []\n",
    "for _ in range(15):\n",
    "    outlier_post_num = random.randint(1, num_posts)\n",
    "    outlier_post_id = f'post_{outlier_post_num}'\n",
    "    outlier_platform = random.choice(platforms)\n",
    "    outlier_content_type = random.choice(content_types)\n",
    "    outlier_content_theme = random.choice(content_themes)\n",
    "    outlier_date = random.choice(date_range).strftime('%Y-%m-%d')\n",
    "    \n",
    "    outlier_record = {\n",
    "        'post_id': outlier_post_id,\n",
    "        'platform': outlier_platform,\n",
    "        'content_type': outlier_content_type,\n",
    "        'content_theme': outlier_content_theme,\n",
    "        'date_posted': outlier_date,\n",
    "        'organic_impressions': random.randint(1000000, 5000000),  # Extremely high organic impressions\n",
    "        'paid_impressions': random.randint(500000, 2000000),     # Extremely high paid impressions\n",
    "        'organic_engagements': random.randint(50000, 300000),    # Extremely high organic engagements\n",
    "        'paid_engagements': random.randint(25000, 150000),       # Extremely high paid engagements\n",
    "        'clicks': random.randint(5000, 100000),                  # Extremely high clicks\n",
    "        'reach': random.randint(1000000, 5000000),               # Extremely high reach\n",
    "        'followers_gain': random.randint(1000, 5000),            # Extremely high followers gain\n",
    "        'conversions': random.randint(500, 3000),                # Extremely high conversions\n",
    "        'sentiment_score': round(random.uniform(0.5, 1.0), 2),   # Highly positive sentiment\n",
    "        'country': random.choice(countries)                      # Assign country\n",
    "    }\n",
    "    outlier_records.append(outlier_record)\n",
    "\n",
    "# Convert outliers to DataFrame\n",
    "outliers_df = pd.DataFrame(outlier_records)\n",
    "\n",
    "# Concatenate outliers with the main DataFrame\n",
    "social_media_df = pd.concat([social_media_df, outliers_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the DataFrame to mix outliers and duplicates\n",
    "social_media_df = social_media_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Parse 'date_posted' column to datetime for sorting\n",
    "social_media_df['parsed_date'] = pd.to_datetime(social_media_df['date_posted'], dayfirst=True, errors='coerce')\n",
    "\n",
    "# Sort by 'parsed_date'\n",
    "social_media_df = social_media_df.sort_values(by='parsed_date').drop('parsed_date', axis=1).reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "social_media_df.to_csv('social_media_dirty.csv', index=False)\n",
    "print(\"Dirty Social Media Data Generated:\")\n",
    "print(social_media_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advertising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86239/1503101099.py:90: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  advertising_df['parsed_date'] = pd.to_datetime(advertising_df['date'], dayfirst=True, errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dirty Advertising Metrics Data Generated:\n",
      "    ad_id       platform        date  impressions  clicks  cost_usd  \\\n",
      "0  ad_100           None  2024-07-21         5998      96   2079.73   \n",
      "1   ad_49  Instagram Ads  2024-07-21         4711     442   2211.19   \n",
      "2   ad_63     Google Ads  2024-07-21         4887     386   3659.98   \n",
      "3   ad_29      Insta Ads  2024-07-21         4754     496   2439.85   \n",
      "4   ad_72   Facebook Ads  2024-07-21         8379     222   3619.68   \n",
      "\n",
      "   conversions  \n",
      "0           19  \n",
      "1           16  \n",
      "2          -79  \n",
      "3           54  \n",
      "4           96  \n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "fake.seed_instance(42)\n",
    "\n",
    "# Parameters\n",
    "num_ads = 100\n",
    "num_records_per_ad = 1000\n",
    "total_days = 90  # Approximately 3 months\n",
    "\n",
    "# Initialize list to hold all advertising records\n",
    "advertising = []\n",
    "\n",
    "# Define platform names with inconsistencies\n",
    "ad_platforms = ['Google Ads', 'Facebook Ads', 'Instagram Ads', 'LinkedIn Ads', 'GAds', 'FB Ads', 'Insta Ads']\n",
    "\n",
    "# Generate a list of dates for the past 3 months\n",
    "date_range = pd.date_range(end=pd.Timestamp.today(), periods=total_days).tolist()\n",
    "\n",
    "# Generate Advertising Metrics Data\n",
    "ad_ids = [f'ad_{i+1}' for i in range(num_ads)]\n",
    "\n",
    "for ad_id in ad_ids:\n",
    "    # Assign a base date for the ad to introduce some trends\n",
    "    base_date = fake.date_between(start_date='-3m', end_date='today')\n",
    "    \n",
    "    for _ in range(num_records_per_ad):\n",
    "        # Select a date from the date range\n",
    "        date = random.choice(date_range)\n",
    "        \n",
    "        # Optionally, introduce some trend by slightly adjusting the date\n",
    "        # For example, more records on weekdays vs weekends\n",
    "        if date.weekday() < 5:  # Weekday\n",
    "            date_str = date.strftime('%Y-%m-%d')\n",
    "        else:  # Weekend\n",
    "            date_str = date.strftime('%d/%m/%Y') if random.random() < 0.1 else date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Introduce missing values (5% for 'platform')\n",
    "        platform = random.choice(ad_platforms) if random.random() > 0.05 else None\n",
    "        \n",
    "        # Introduce erroneous entries (2% negative values)\n",
    "        impressions = random.randint(100, 10000) if random.random() > 0.02 else -random.randint(100, 10000)\n",
    "        clicks = random.randint(0, 500) if random.random() > 0.02 else -random.randint(0, 500)\n",
    "        cost_usd = round(random.uniform(50, 5000), 2) if random.random() > 0.02 else -round(random.uniform(50, 5000), 2)\n",
    "        conversions = random.randint(0, 100) if random.random() > 0.02 else -random.randint(0, 100)\n",
    "        \n",
    "        ad_record = {\n",
    "            'ad_id': ad_id,\n",
    "            'platform': platform,\n",
    "            'date': date_str,\n",
    "            'impressions': impressions,\n",
    "            'clicks': clicks,\n",
    "            'cost_usd': cost_usd,\n",
    "            'conversions': conversions\n",
    "        }\n",
    "        \n",
    "        advertising.append(ad_record)\n",
    "\n",
    "# Convert to DataFrame\n",
    "advertising_df = pd.DataFrame(advertising)\n",
    "\n",
    "# Introduce Duplicates (2% of the data)\n",
    "duplicates = advertising_df.sample(frac=0.02, random_state=42)\n",
    "advertising_df = pd.concat([advertising_df, duplicates], ignore_index=True)\n",
    "\n",
    "# Introduce Additional Outliers by adding some extreme values (50 records)\n",
    "outlier_records = []\n",
    "for _ in range(50):\n",
    "    outlier_date = random.choice(date_range).strftime('%Y-%m-%d')\n",
    "    outlier_record = {\n",
    "        'ad_id': random.choice(ad_ids),\n",
    "        'platform': random.choice(ad_platforms),\n",
    "        'date': outlier_date,\n",
    "        'impressions': random.randint(10000, 1000000),  # Extremely high impressions\n",
    "        'clicks': random.randint(1000, 10000),            # Extremely high clicks\n",
    "        'cost_usd': round(random.uniform(10000, 100000), 2),  # Extremely high cost\n",
    "        'conversions': random.randint(100, 1000)        # Extremely high conversions\n",
    "    }\n",
    "    outlier_records.append(outlier_record)\n",
    "\n",
    "# Convert outliers to DataFrame\n",
    "outliers_df = pd.DataFrame(outlier_records)\n",
    "\n",
    "# Concatenate outliers with the main DataFrame\n",
    "advertising_df = pd.concat([advertising_df, outliers_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the DataFrame to mix outliers and duplicates\n",
    "advertising_df = advertising_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Optionally, sort by date to simulate a time series\n",
    "advertising_df['parsed_date'] = pd.to_datetime(advertising_df['date'], dayfirst=True, errors='coerce')\n",
    "advertising_df = advertising_df.sort_values(by='parsed_date').drop('parsed_date', axis=1).reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "advertising_df.to_csv('advertising_metrics_dirty.csv', index=False)\n",
    "print(\"Dirty Advertising Metrics Data Generated:\")\n",
    "print(advertising_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marketing-analytics-uQXf7Hmg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
